"""
change_detector.py
==================
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ—Ä—Ç–∞–ª–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∫–∞–∑–æ–≤.

–ó–∞–¥–∞—á–∏:
  1. –û–±—Ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü-—Å–ø–∏—Å–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–∞–∂–¥–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É
  2. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞? (—Ö–µ—à DOM / ETag)
  3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
  4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø—Ä–∏–∫–∞–∑–∞–º–∏ –≤ –ë–î
  5. –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –Ω–æ–≤—ã—Ö –ø—Ä–∏–∫–∞–∑–æ–≤ (—Å—Ç–∞—Ç—É—Å 'new') –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
  6. –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π: –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã DOM (–∞–ª–µ—Ä—Ç)

–¢–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
  - pdf_portal:   HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ PDF
                  (mos.ru, mst.mosreg.ru, kfis.gov.spb.ru, minsport.krasnodar.ru)
  - json_embed:   JSON-–¥–∞–Ω–Ω—ã–µ –≤—Å—Ç—Ä–æ–µ–Ω—ã –≤ JS-–ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                  (msrfinfo.ru)
  - html_table:   HTML-—Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ (–±—É–¥—É—â–∏–π —Ç–∏–ø)

–°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ risk_class:
  - green:  httpx, 1‚Äì3—Å –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, ETag/If-Modified-Since
  - amber:  Playwright, 3‚Äì8—Å, —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏, change detection
  - red:    —Ç–æ–ª—å–∫–æ —Ä—É—á–Ω–æ–π –∏–º–ø–æ—Ä—Ç, –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –Ω–æ–≤–æ–≥–æ

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    detector = ChangeDetector(db_url="postgresql+asyncpg://...", pdf_output_dir="./pdfs")
    await detector.initialize()

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫
    result = await detector.check_source("spb_kfkis")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    results = await detector.check_all()

    # –ó–∞–ø—É—Å–∫ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    await detector.run_loop(interval_minutes=60)

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
    pip install httpx sqlalchemy[asyncio] asyncpg
    (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) pip install playwright  ‚Äî –¥–ª—è amber-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
"""

import asyncio
import hashlib
import json
import logging
import random
import re
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Optional, Union
from urllib.parse import urljoin, urlparse, parse_qs

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
# ---------------------------------------------------------------------------

class CheckStatus(str, Enum):
    UNCHANGED   = "unchanged"      # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å (—Ö–µ—à —Å–æ–≤–ø–∞–ª)
    NEW_DOCS    = "new_docs"       # –ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    CHANGED     = "changed"        # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –Ω–æ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç
    ERROR       = "error"          # –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ
    SKIPPED     = "skipped"        # –ü—Ä–æ–ø—É—â–µ–Ω (red/inactive/rate limit)


@dataclass
class DiscoveredDocument:
    """–û–¥–∏–Ω –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
    url: str                       # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ PDF
    file_url: Optional[str] = None # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ PDF (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–∞)
    title: Optional[str] = None    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Å—ã–ª–∫–∏/–∫–∞—Ä—Ç–æ—á–∫–∏
    order_number: str = ""         # –ù–æ–º–µ—Ä –ø—Ä–∏–∫–∞–∑–∞ (–µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å)
    order_date: str = ""           # –î–∞—Ç–∞ –ø—Ä–∏–∫–∞–∑–∞ (–µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å)
    order_type: str = "–ø—Ä–∏–∫–∞–∑"     # –ø—Ä–∏–∫–∞–∑/—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ


@dataclass
class CheckResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
    source_code: str
    status: CheckStatus
    duration_ms: int = 0

    # –ß—Ç–æ –Ω–∞—à–ª–∏
    page_hash: Optional[str] = None
    page_hash_changed: bool = False
    links_total: int = 0
    links_new: int = 0
    orders_created: int = 0

    # –î–æ–∫—É–º–µ–Ω—Ç—ã
    discovered: list[DiscoveredDocument] = field(default_factory=list)
    new_documents: list[DiscoveredDocument] = field(default_factory=list)

    # –û—à–∏–±–∫–∞
    error: Optional[str] = None
    etag: Optional[str] = None

    def summary(self) -> str:
        if self.status == CheckStatus.NEW_DOCS:
            return (
                f"üÜï {self.source_code}: {self.links_new} –Ω–æ–≤—ã—Ö –∏–∑ {self.links_total} "
                f"({self.orders_created} —Å–æ–∑–¥–∞–Ω–æ –≤ –ë–î), {self.duration_ms}ms"
            )
        if self.status == CheckStatus.UNCHANGED:
            return f"‚úÖ {self.source_code}: –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, {self.duration_ms}ms"
        if self.status == CheckStatus.CHANGED:
            return f"üîÑ {self.source_code}: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –Ω–æ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç"
        if self.status == CheckStatus.ERROR:
            return f"‚ùå {self.source_code}: {self.error}"
        return f"‚è≠Ô∏è {self.source_code}: –ø—Ä–æ–ø—É—â–µ–Ω"


# ---------------------------------------------------------------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ —Ç–∏–ø–∞–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
# ---------------------------------------------------------------------------

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã URL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
# –ó–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ source_registry (–µ–¥–∏–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä)
try:
    from source_registry import as_detect_patterns as _load_patterns
    SOURCE_PATTERNS: dict[str, dict] = _load_patterns()
except ImportError:
    SOURCE_PATTERNS: dict[str, dict] = {
        "moskva_tstisk": {
            "list_urls": [
                "https://www.mos.ru/moskomsport/documents/prisvoenie-sportivnykh-razryadov-po-vidam-sporta/",
                "https://www.mos.ru/moskomsport/documents/prisvoenie-kvalifikatsionnykh-kategoriy-sportivnykh-sudey/",
            ],
            "link_regex": r'href=["\']([^"\']*view/\d+[^"\']*)["\']',
            "title_regex": r'>([^<]*(?:–ü—Ä–∏–∫–∞–∑|–†–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ)[^<]*)<',
            "order_date_regex": r'–æ—Ç\s+(\d{1,2}[\.\s]\d{2}[\.\s]\d{4})',
            "order_number_regex": r'[‚ÑñN]\s*(\S+)',
            "method": "playwright",
            "pagination": None,
        },
        "moskva_moskumsport": {
            "list_urls": [
                "https://www.mos.ru/moskomsport/documents/prisvoenie-sportivnykh-razryadov-po-vidam-sporta/",
            ],
            "link_regex": r'href=["\']([^"\']*view/\d+[^"\']*)["\']',
            "title_regex": r'>([^<]*–†–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ[^<]*)<',
            "method": "playwright",
            "pagination": None,
        },
        "mo_m–æ—Å–æ–±–ª—Å–ø–æ—Ä—Ç": {
            "list_urls": [
                "https://mst.mosreg.ru/dokumenty/prisvoenie-sportivnykh-razryadov-kandidat-v-mastera-sporta-i-pervyi-sportivnyi-razryad",
            ],
            "link_regex": r'href=["\']([^"\']*(?:rasporiaz|prikaz)[^"\']*)["\']',
            "method": "playwright",
            "pagination": "?page={n}",
            "max_pages": 3,
        },
        "spb_kfkis": {
            "list_urls": [
                "https://kfis.gov.spb.ru/docs/?type=54",
            ],
            "link_regex": r'href=["\']([^"\']*?(?:/docs/\d+|/documents/\d+)[^"\']*)["\']',
            "title_regex": r'class=["\']doc-title["\'][^>]*>([^<]+)<',
            "method": "httpx",
            "pagination": "&page={n}",
            "max_pages": 3,
        },
        "krasnodar_minsport": {
            "list_urls": [
                "https://minsport.krasnodar.ru/activities/sport/prisvoenie-sportivnyx-razryadov/",
            ],
            "link_regex": r'href=["\']([^"\']*\.pdf)["\']',
            "method": "httpx",
            "pagination": None,
        },
        "rf_minsport": {
            "list_urls": [
                "https://msrfinfo.ru/awards/",
            ],
            "method": "httpx",
            "source_type": "json_embed",
            "js_var": "$obj",
        },
    }

# User-agents (—Å–æ–≤–ø–∞–¥–∞—é—Ç —Å pdf_downloader)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
]


# ---------------------------------------------------------------------------
# Change Detector
# ---------------------------------------------------------------------------

class ChangeDetector:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—Ç–∞–ª—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∫–∞–∑–æ–≤.

    –ê–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞:
    1. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É-—Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    2. –í—ã—á–∏—Å–ª–∏—Ç—å —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ‚Üí —Å—Ä–∞–≤–Ω–∏—Ç—å —Å last_page_hash
       - –ï—Å–ª–∏ –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è –∏ last_checked_at < 1 —á –Ω–∞–∑–∞–¥ ‚Üí UNCHANGED
    3. –ò–∑–≤–ª–µ—á—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    4. –°—Ä–∞–≤–Ω–∏—Ç—å —Å —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ URL –≤ orders
    5. –î–ª—è –Ω–æ–≤—ã—Ö ‚Üí —Å–æ–∑–¥–∞—Ç—å –∑–∞–ø–∏—Å–∏ –≤ orders (status='new')
    6. –û–±–Ω–æ–≤–∏—Ç—å last_page_hash, last_etag, last_checked_at
    """

    def __init__(
        self,
        db_url: Optional[str] = None,
        pdf_output_dir: str = "./pdfs",
        playwright_headless: bool = True,
        request_timeout: float = 30.0,
    ):
        self._db_url = db_url
        self._pdf_dir = pdf_output_dir
        self._headless = playwright_headless
        self._timeout = request_timeout

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._engine = None
        self._browser = None
        self._playwright = None
        self._initialized = False
        self._browser_sem = asyncio.Semaphore(2)  # –º–∞–∫—Å. 2 Playwright —Å–µ—Å—Å–∏–∏

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ë–î –∏ HTTP –∫–ª–∏–µ–Ω—Ç."""
        # DB
        if self._db_url:
            try:
                from sqlalchemy.ext.asyncio import create_async_engine
                self._engine = create_async_engine(
                    self._db_url, pool_size=3, echo=False,
                )
                async with self._engine.begin() as conn:
                    from sqlalchemy import text
                    await conn.execute(text("SELECT 1"))
                logger.info("ChangeDetector: –ë–î –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
            except Exception as e:
                logger.error(f"ChangeDetector: –æ—à–∏–±–∫–∞ –ë–î ‚Äî {e}")
                self._engine = None
        else:
            logger.info("ChangeDetector: —Ä–∞–±–æ—Ç–∞ –±–µ–∑ –ë–î (dry-run)")

        self._initialized = True

    async def shutdown(self):
        """–û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã."""
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()
        if self._engine:
            await self._engine.dispose()

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, *args):
        await self.shutdown()

    # ------------------------------------------------------------------
    # –ü—É–±–ª–∏—á–Ω—ã–π API
    # ------------------------------------------------------------------

    async def check_source(self, source_code: str) -> CheckResult:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞ –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.
        """
        if not self._initialized:
            raise RuntimeError("–ù–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –í—ã–∑–æ–≤–∏—Ç–µ await initialize()")

        t0 = time.monotonic()

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source_info = await self._get_source_info(source_code)
        pattern = SOURCE_PATTERNS.get(source_code, {})

        if not pattern:
            return CheckResult(
                source_code=source_code,
                status=CheckStatus.SKIPPED,
                error=f"–ù–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ '{source_code}'",
            )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º risk_class
        risk = source_info.get("risk_class", "green") if source_info else "green"
        if risk == "red":
            logger.info(f"[{source_code}] risk=red, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–≤–µ—Ä–∫—É")
            return CheckResult(
                source_code=source_code,
                status=CheckStatus.SKIPPED,
                error="risk_class=red, —Ç–æ–ª—å–∫–æ —Ä—É—á–Ω–æ–π –∏–º–ø–æ—Ä—Ç",
            )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        try:
            result = await self._check_source_impl(source_code, pattern, source_info)
            result.duration_ms = int((time.monotonic() - t0) * 1000)
            logger.info(result.summary())
            return result
        except Exception as e:
            logger.error(f"[{source_code}] –æ—à–∏–±–∫–∞: {e}")
            return CheckResult(
                source_code=source_code,
                status=CheckStatus.ERROR,
                error=str(e),
                duration_ms=int((time.monotonic() - t0) * 1000),
            )

    async def check_all(
        self,
        source_codes: Optional[list[str]] = None,
        skip_red: bool = True,
    ) -> list[CheckResult]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ (–∏–ª–∏ —É–∫–∞–∑–∞–Ω–Ω—ã–µ) –∞–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
        –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, —Å –ø–∞—É–∑–∞–º–∏ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.
        """
        if source_codes is None:
            source_codes = await self._get_active_source_codes()

        if not source_codes:
            # Fallback: –≤—Å–µ –∏–∑ SOURCE_PATTERNS
            source_codes = list(SOURCE_PATTERNS.keys())

        results = []
        for i, code in enumerate(source_codes):
            if i > 0:
                delay = random.uniform(2, 5)
                logger.debug(f"–ü–∞—É–∑–∞ {delay:.1f}—Å –ø–µ—Ä–µ–¥ {code}")
                await asyncio.sleep(delay)

            result = await self.check_source(code)
            results.append(result)

        # –°–≤–æ–¥–∫–∞
        new = sum(r.links_new for r in results)
        created = sum(r.orders_created for r in results)
        errors = sum(1 for r in results if r.status == CheckStatus.ERROR)
        logger.info(
            f"ChangeDetector: –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {len(results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, "
            f"{new} –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫, {created} —Å–æ–∑–¥–∞–Ω–æ, {errors} –æ—à–∏–±–æ–∫"
        )

        return results

    async def run_loop(
        self,
        interval_minutes: int = 60,
        source_codes: Optional[list[str]] = None,
    ):
        """
        –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º.
        –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞: –∑–∞–ø—É—Å–∫–∞—Ç—å —á–µ—Ä–µ–∑ supervisor/systemd.
        """
        logger.info(
            f"ChangeDetector: –∑–∞–ø—É—Å–∫ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ "
            f"(–∏–Ω—Ç–µ—Ä–≤–∞–ª={interval_minutes}–º–∏–Ω)"
        )
        while True:
            try:
                await self.check_all(source_codes)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")

            logger.info(f"–°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ {interval_minutes} –º–∏–Ω")
            await asyncio.sleep(interval_minutes * 60)

    # ------------------------------------------------------------------
    # –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ª–æ–≥–∏–∫–∞
    # ------------------------------------------------------------------

    async def _check_source_impl(
        self,
        source_code: str,
        pattern: dict,
        source_info: Optional[dict],
    ) -> CheckResult:
        """–û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        result = CheckResult(source_code=source_code, status=CheckStatus.UNCHANGED)

        list_urls = pattern.get("list_urls", [])
        method = pattern.get("method", "httpx")
        source_type = pattern.get("source_type", "pdf_portal")

        all_discovered: list[DiscoveredDocument] = []

        for list_url in list_urls:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É(—ã) ‚Äî —Å —É—á—ë—Ç–æ–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            pages_html = await self._fetch_list_pages(
                list_url, method, pattern
            )

            for page_url, html, etag in pages_html:
                # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à
                content_hash = self._hash_content(html)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
                if source_type == "json_embed":
                    docs = self._extract_json_embed(html, page_url, pattern)
                else:
                    docs = self._extract_pdf_links(html, page_url, pattern)

                all_discovered.extend(docs)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–µ—à –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if result.page_hash is None:
                    result.page_hash = content_hash
                    result.etag = etag

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
        seen_urls = set()
        unique_docs = []
        for doc in all_discovered:
            url_key = doc.file_url or doc.url
            if url_key not in seen_urls:
                seen_urls.add(url_key)
                unique_docs.append(doc)

        result.discovered = unique_docs
        result.links_total = len(unique_docs)

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ë–î ‚Äî –∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã?
        known_urls = await self._get_known_urls(source_code)
        new_docs = []
        for doc in unique_docs:
            url_key = doc.file_url or doc.url
            if url_key not in known_urls and doc.url not in known_urls:
                new_docs.append(doc)

        result.new_documents = new_docs
        result.links_new = len(new_docs)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ö–µ—à —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        old_hash = source_info.get("last_page_hash") if source_info else None
        if old_hash and result.page_hash:
            result.page_hash_changed = (old_hash != result.page_hash)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if new_docs:
            result.status = CheckStatus.NEW_DOCS
            # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å–∏ –≤ –ë–î
            result.orders_created = await self._create_orders(
                source_code, new_docs
            )
        elif result.page_hash_changed:
            result.status = CheckStatus.CHANGED
        else:
            result.status = CheckStatus.UNCHANGED

        # –û–±–Ω–æ–≤–ª—è–µ–º last_page_hash, last_checked_at
        await self._update_source_check(
            source_code, result.page_hash, result.etag
        )

        return result

    # ------------------------------------------------------------------
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
    # ------------------------------------------------------------------

    async def _fetch_list_pages(
        self,
        base_url: str,
        method: str,
        pattern: dict,
    ) -> list[tuple[str, str, Optional[str]]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–Ω—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–ø–∏—Å–∫–∞ (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [(url, html, etag), ...]
        """
        pages = []

        # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        html, etag = await self._fetch_page(base_url, method)
        pages.append((base_url, html, etag))

        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
        pagination = pattern.get("pagination")
        max_pages = pattern.get("max_pages", 1)

        if pagination and max_pages > 1:
            for page_num in range(2, max_pages + 1):
                page_url = base_url + pagination.format(n=page_num)
                try:
                    delay = random.uniform(1.5, 3.0)
                    await asyncio.sleep(delay)
                    html_page, _ = await self._fetch_page(page_url, method)
                    pages.append((page_url, html_page, None))
                except Exception as e:
                    logger.debug(
                        f"–ü–∞–≥–∏–Ω–∞—Ü–∏—è: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ({e}), "
                        f"–æ—Å—Ç–∞–Ω–æ–≤–∫–∞"
                    )
                    break

        return pages

    async def _fetch_page(
        self, url: str, method: str
    ) -> tuple[str, Optional[str]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (html, etag).
        """
        if method == "playwright":
            return await self._fetch_playwright(url), None
        else:
            return await self._fetch_httpx(url)

    async def _fetch_httpx(self, url: str) -> tuple[str, Optional[str]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ httpx —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ETag."""
        try:
            import httpx
        except ImportError:
            raise ImportError("pip install httpx")

        headers = {
            "User-Agent": random.choice(USER_AGENTS),
            "Accept": "text/html,application/xhtml+xml",
            "Accept-Language": "ru-RU,ru;q=0.9",
        }

        async with httpx.AsyncClient(
            follow_redirects=True,
            timeout=self._timeout,
            headers=headers,
        ) as client:
            resp = await client.get(url)
            resp.raise_for_status()
            etag = resp.headers.get("etag")
            return resp.text, etag

    async def _fetch_playwright(self, url: str) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ Playwright (–¥–ª—è JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ –∞–Ω—Ç–∏–±–æ—Ç–æ–≤)."""
        async with self._browser_sem:  # –º–∞–∫—Å. 2 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å–µ—Å—Å–∏–∏
            browser = await self._ensure_browser()
            context = await browser.new_context(
                user_agent=random.choice(USER_AGENTS),
                locale="ru-RU",
            )
            page = await context.new_page()

            try:
                await page.goto(url, wait_until="networkidle", timeout=60_000)
                # –ñ–¥—ë–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –¥–ª—è JS-–∫–æ–Ω—Ç–µ–Ω—Ç–∞
                await asyncio.sleep(2)
                html = await page.content()
                return html
            finally:
                await page.close()
                await context.close()

    async def _ensure_browser(self):
        """Lazy-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Playwright."""
        if self._browser is None:
            try:
                from playwright.async_api import async_playwright
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=self._headless,
                )
                logger.info("ChangeDetector: Playwright browser –∑–∞–ø—É—â–µ–Ω")
            except ImportError:
                raise ImportError(
                    "pip install playwright && playwright install chromium"
                )
        return self._browser

    # ------------------------------------------------------------------
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    # ------------------------------------------------------------------

    def _extract_pdf_links(
        self,
        html: str,
        page_url: str,
        pattern: dict,
    ) -> list[DiscoveredDocument]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ HTML (pdf_portal)."""
        link_regex = pattern.get("link_regex", r'href=["\']([^"\']*\.pdf)["\']')

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        raw_links = re.findall(link_regex, html, re.IGNORECASE)
        links = list(dict.fromkeys(  # –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
            urljoin(page_url, href) for href in raw_links
        ))

        docs = []
        for link in links:
            doc = DiscoveredDocument(url=link)

            # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –≤–µ–¥—ë—Ç –Ω–∞–ø—Ä—è–º—É—é –Ω–∞ PDF
            if link.lower().endswith(".pdf"):
                doc.file_url = link

            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
            self._enrich_from_html(doc, html, link, pattern)
            docs.append(doc)

        return docs

    def _extract_json_embed(
        self,
        html: str,
        page_url: str,
        pattern: dict,
    ) -> list[DiscoveredDocument]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON, –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –≤ JS-–ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (json_embed)."""
        js_var = pattern.get("js_var", "$obj")

        # –ò—â–µ–º: var $obj = {...}; –∏–ª–∏ $obj = {...};
        regex = rf'{re.escape(js_var)}\s*=\s*(\{{.*?\}});'
        match = re.search(regex, html, re.DOTALL)
        if not match:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: –º–∞—Å—Å–∏–≤
            regex = rf'{re.escape(js_var)}\s*=\s*(\[.*?\]);'
            match = re.search(regex, html, re.DOTALL)

        if not match:
            logger.warning(f"json_embed: –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{js_var}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []

        try:
            data = json.loads(match.group(1))
        except json.JSONDecodeError as e:
            logger.warning(f"json_embed: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON ‚Äî {e}")
            return []

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º JSON –≤ DiscoveredDocument
        docs = []
        items = data if isinstance(data, list) else data.get("data", data.get("items", []))
        if isinstance(items, dict):
            items = list(items.values()) if all(isinstance(v, dict) for v in items.values()) else [items]

        for item in items:
            if not isinstance(item, dict):
                continue
            doc = DiscoveredDocument(
                url=urljoin(page_url, str(item.get("url", item.get("link", "")))),
                title=item.get("title", item.get("name", "")),
                order_number=str(item.get("number", item.get("order_number", ""))),
                order_date=str(item.get("date", item.get("order_date", ""))),
            )
            if doc.url and doc.url != page_url:
                docs.append(doc)

        return docs

    @staticmethod
    def _enrich_from_html(
        doc: DiscoveredDocument,
        html: str,
        link: str,
        pattern: dict,
    ):
        """–û–±–æ–≥–∞—â–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–∑ HTML-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏."""
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—Å—ã–ª–∫–∏ (¬±500 —Å–∏–º–≤–æ–ª–æ–≤)
        idx = html.find(link)
        if idx < 0:
            # –°—Å—ã–ª–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–µ–∑ –¥–æ–º–µ–Ω–∞ –≤ HTML
            parsed = urlparse(link)
            short = parsed.path
            idx = html.find(short)

        if idx >= 0:
            ctx = html[max(0, idx - 500):idx + 500]

            # –ù–æ–º–µ—Ä –ø—Ä–∏–∫–∞–∑–∞
            num_regex = pattern.get(
                "order_number_regex",
                r'[‚ÑñN]\s*([–ê-–Ø–∞-—èA-Za-z0-9\-/]+)'
            )
            m = re.search(num_regex, ctx)
            if m and not doc.order_number:
                doc.order_number = m.group(1).strip()

            # –î–∞—Ç–∞
            date_regex = pattern.get(
                "order_date_regex",
                r'(\d{2}[\.\s]\d{2}[\.\s]\d{4})'
            )
            m = re.search(date_regex, ctx)
            if m and not doc.order_date:
                doc.order_date = m.group(1).replace(" ", ".")

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_regex = pattern.get("title_regex")
            if title_regex:
                m = re.search(title_regex, ctx)
                if m and not doc.title:
                    doc.title = m.group(1).strip()

            # –¢–∏–ø: –ø—Ä–∏–∫–∞–∑ –∏–ª–∏ —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ
            ctx_lower = ctx.lower()
            if "—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ" in ctx_lower:
                doc.order_type = "—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ"
            elif "–ø—Ä–∏–∫–∞–∑" in ctx_lower:
                doc.order_type = "–ø—Ä–∏–∫–∞–∑"

    # ------------------------------------------------------------------
    # –†–∞–±–æ—Ç–∞ —Å –ë–î
    # ------------------------------------------------------------------

    async def _get_source_info(self, source_code: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ –∏–∑ –ë–î."""
        if not self._engine:
            return None

        from sqlalchemy import text

        async with self._engine.begin() as conn:
            row = await conn.execute(
                text("""
                    SELECT id, code, source_type, risk_class,
                           last_page_hash, last_etag, last_checked_at,
                           discovery_config, active
                    FROM registry_sources
                    WHERE code = :code
                """),
                {"code": source_code},
            )
            r = row.fetchone()
            if r:
                return dict(r._mapping)
        return None

    async def _get_active_source_codes(self) -> list[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–¥—ã –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –ë–î."""
        if not self._engine:
            return list(SOURCE_PATTERNS.keys())

        from sqlalchemy import text

        async with self._engine.begin() as conn:
            rows = await conn.execute(
                text("SELECT code FROM registry_sources WHERE active = TRUE ORDER BY code")
            )
            return [r[0] for r in rows.fetchall()]

    async def _get_known_urls(self, source_code: str) -> set[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö URL –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        if not self._engine:
            return set()

        from sqlalchemy import text

        async with self._engine.begin() as conn:
            # –ù–∞—Ö–æ–¥–∏–º source_id
            row = await conn.execute(
                text("SELECT id FROM registry_sources WHERE code = :code"),
                {"code": source_code},
            )
            r = row.fetchone()
            if not r:
                return set()
            source_id = str(r[0])

            # –í—Å–µ URL –∏–∑ orders –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            rows = await conn.execute(
                text("""
                    SELECT source_url, file_url
                    FROM orders
                    WHERE source_id = :sid
                """),
                {"sid": source_id},
            )
            urls = set()
            for r in rows.fetchall():
                if r[0]:
                    urls.add(r[0])
                if r[1]:
                    urls.add(r[1])
            return urls

    async def _create_orders(
        self,
        source_code: str,
        docs: list[DiscoveredDocument],
    ) -> int:
        """–°–æ–∑–¥–∞—ë—Ç –∑–∞–ø–∏—Å–∏ –ø—Ä–∏–∫–∞–∑–æ–≤ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        if not self._engine:
            logger.info(
                f"[{source_code}] dry-run: —Å–æ–∑–¥–∞–Ω–æ –±—ã {len(docs)} –ø—Ä–∏–∫–∞–∑–æ–≤"
            )
            return len(docs)

        from sqlalchemy import text
        from uuid import uuid4

        async with self._engine.begin() as conn:
            # source_id
            row = await conn.execute(
                text("SELECT id FROM registry_sources WHERE code = :code"),
                {"code": source_code},
            )
            r = row.fetchone()
            if not r:
                logger.error(f"–ò—Å—Ç–æ—á–Ω–∏–∫ '{source_code}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return 0
            source_id = str(r[0])

            created = 0
            for doc in docs:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ URL
                    existing = await conn.execute(
                        text("""
                            SELECT id FROM orders
                            WHERE source_id = :sid AND (
                                source_url = :url OR file_url = :furl
                            )
                            LIMIT 1
                        """),
                        {"sid": source_id, "url": doc.url, "furl": doc.file_url},
                    )
                    if existing.fetchone():
                        continue

                    order_id = str(uuid4())
                    await conn.execute(
                        text("""
                            INSERT INTO orders (
                                id, source_id, order_number, order_date,
                                order_type, title, source_url, file_url, status
                            ) VALUES (
                                :id, :sid, :num, :dt,
                                :type, :title, :surl, :furl, 'new'
                            )
                        """),
                        {
                            "id": order_id,
                            "sid": source_id,
                            "num": doc.order_number or "",
                            "dt": doc.order_date or "",
                            "type": doc.order_type,
                            "title": doc.title,
                            "surl": doc.url,
                            "furl": doc.file_url,
                        },
                    )
                    created += 1
                    logger.info(
                        f"[{source_code}] üìÑ –ù–æ–≤—ã–π: "
                        f"{doc.order_number or doc.url[:60]}"
                    )

                except Exception as e:
                    logger.warning(
                        f"[{source_code}] –æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏: {e}"
                    )

            return created

    async def _update_source_check(
        self,
        source_code: str,
        page_hash: Optional[str],
        etag: Optional[str],
    ):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç last_page_hash, last_etag, last_checked_at."""
        if not self._engine:
            return

        from sqlalchemy import text

        fields = ["last_checked_at = NOW()"]
        params: dict = {"code": source_code}

        if page_hash:
            fields.append("last_page_hash = :hash")
            params["hash"] = page_hash
        if etag:
            fields.append("last_etag = :etag")
            params["etag"] = etag

        sql = f"UPDATE registry_sources SET {', '.join(fields)} WHERE code = :code"

        async with self._engine.begin() as conn:
            await conn.execute(text(sql), params)

    async def _log_to_db(
        self,
        source_code: str,
        level: str,
        message: str,
        details: Optional[dict] = None,
    ):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å–æ–±—ã—Ç–∏–µ –≤ processing_log."""
        if not self._engine:
            return

        from sqlalchemy import text
        from uuid import uuid4

        async with self._engine.begin() as conn:
            row = await conn.execute(
                text("SELECT id FROM registry_sources WHERE code = :code"),
                {"code": source_code},
            )
            r = row.fetchone()
            source_id = str(r[0]) if r else None

            await conn.execute(
                text("""
                    INSERT INTO processing_log
                    (id, source_id, level, stage, message, details)
                    VALUES (:id, :sid, :level, 'change_detection', :msg, :det::jsonb)
                """),
                {
                    "id": str(uuid4()),
                    "sid": source_id,
                    "level": level,
                    "msg": message[:2000],
                    "det": json.dumps(details or {}, ensure_ascii=False),
                },
            )

    # ------------------------------------------------------------------
    # –£—Ç–∏–ª–∏—Ç—ã
    # ------------------------------------------------------------------

    @staticmethod
    def _hash_content(html: str) -> str:
        """
        –•–µ—à ¬´–∑–Ω–∞—á–∏–º–æ–π¬ª —á–∞—Å—Ç–∏ HTML.
        –£–±–∏—Ä–∞–µ—Ç: –ø—Ä–æ–±–µ–ª—ã, timestamps, nonce, csrf, session tokens ‚Äî
        —á—Ç–æ–±—ã —Ö–µ—à –º–µ–Ω—è–ª—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ä–µ–∞–ª—å–Ω–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
        """
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏ (—á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç nonce/timestamps)
        cleaned = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        cleaned = re.sub(r'<style[^>]*>.*?</style>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)

        # –£–¥–∞–ª—è–µ–º HTML-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        cleaned = re.sub(r'<!--.*?-->', '', cleaned, flags=re.DOTALL)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()

        # –£–¥–∞–ª—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        cleaned = re.sub(r'(csrf|nonce|token|session|timestamp)=["\'][^"\']*["\']',
                         '', cleaned, flags=re.IGNORECASE)

        return hashlib.sha256(cleaned.encode("utf-8")).hexdigest()


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

async def _main():
    import argparse

    parser = argparse.ArgumentParser(
        description="SportRank Change Detector ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∫–∞–∑–æ–≤"
    )
    sub = parser.add_subparsers(dest="command")

    # check
    p_check = sub.add_parser("check", help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫")
    p_check.add_argument("source", help="–ö–æ–¥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (spb_kfkis, ...)")
    p_check.add_argument("--db", help="PostgreSQL URL")

    # check-all
    p_all = sub.add_parser("check-all", help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
    p_all.add_argument("--db", help="PostgreSQL URL")

    # loop
    p_loop = sub.add_parser("loop", help="–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
    p_loop.add_argument("--db", required=True, help="PostgreSQL URL")
    p_loop.add_argument("--interval", type=int, default=60, help="–ò–Ω—Ç–µ—Ä–≤–∞–ª –≤ –º–∏–Ω—É—Ç–∞—Ö")

    # test-extract
    p_test = sub.add_parser("test-extract", help="–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –∏–∑ HTML")
    p_test.add_argument("source", help="–ö–æ–¥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞")
    p_test.add_argument("html_file", help="HTML-—Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
    )

    if args.command == "check":
        async with ChangeDetector(db_url=getattr(args, "db", None)) as det:
            result = await det.check_source(args.source)
            _print_check_result(result)

    elif args.command == "check-all":
        async with ChangeDetector(db_url=getattr(args, "db", None)) as det:
            results = await det.check_all()
            for r in results:
                _print_check_result(r)

    elif args.command == "loop":
        async with ChangeDetector(db_url=args.db) as det:
            await det.run_loop(interval_minutes=args.interval)

    elif args.command == "test-extract":
        det = ChangeDetector()
        html = Path(args.html_file).read_text(encoding="utf-8")
        pattern = SOURCE_PATTERNS.get(args.source, {})
        docs = det._extract_pdf_links(html, "http://example.com", pattern)
        print(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
        for doc in docs[:20]:
            print(f"  üìÑ {doc.order_number or '?'} –æ—Ç {doc.order_date or '?'}")
            print(f"     URL: {doc.url[:80]}")
            if doc.title:
                print(f"     –ó–∞–≥–æ–ª–æ–≤–æ–∫: {doc.title[:60]}")
            print()

    else:
        parser.print_help()


def _print_check_result(result: CheckResult):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏."""
    print()
    print(result.summary())
    print(f"  –•–µ—à —Å—Ç—Ä–∞–Ω–∏—Ü—ã:   {(result.page_hash or '')[:16]}...")
    print(f"  –•–µ—à –∏–∑–º–µ–Ω–∏–ª—Å—è:  {'–¥–∞' if result.page_hash_changed else '–Ω–µ—Ç'}")
    print(f"  –°—Å—ã–ª–æ–∫ –≤—Å–µ–≥–æ:   {result.links_total}")
    print(f"  –ù–æ–≤—ã—Ö:          {result.links_new}")
    print(f"  –°–æ–∑–¥–∞–Ω–æ –≤ –ë–î:   {result.orders_created}")
    print(f"  –í—Ä–µ–º—è:          {result.duration_ms}ms")

    if result.new_documents:
        print(f"\n  –ù–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
        for doc in result.new_documents[:10]:
            title = doc.title or doc.order_number or doc.url[:60]
            print(f"    üìÑ {title}")
            if doc.order_date:
                print(f"       –î–∞—Ç–∞: {doc.order_date}")
            print(f"       URL: {(doc.file_url or doc.url)[:80]}")

    if result.error:
        print(f"\n  ‚ö†Ô∏è –û—à–∏–±–∫–∞: {result.error}")


if __name__ == "__main__":
    asyncio.run(_main())
